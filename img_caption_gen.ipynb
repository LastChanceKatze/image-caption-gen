{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "img-caption-gen.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8EOZidkRHO44"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LastChanceKatze/image-caption-gen/blob/main/img_caption_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEHDWYoigbhC"
      },
      "source": [
        "# ***Imports***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhheFCoXN3v2"
      },
      "source": [
        "from os import listdir\n",
        "import string\n",
        "from pickle import dump, load\n",
        "import tensorflow.keras.applications.vgg16 as vgg16\n",
        "import tensorflow.keras.applications.inception_v3 as inception_v3\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02GJ0XL47wBr"
      },
      "source": [
        "drive_folder = \"/content/drive/MyDrive/DL\"\n",
        "img_features_path = f\"{drive_folder}/training_files/img_features.pkl\"\n",
        "img_train_path = f\"{drive_folder}/Dataset/Flickr8k_text/Flickr_8k.trainImages.txt\"\n",
        "img_test_path = f\"{drive_folder}/Dataset/Flickr8k_text/Flickr_8k.devImages.txt\"\n",
        "captions_filename = f\"{drive_folder}/training_files/captions.txt\""
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEy8DbU3giqo"
      },
      "source": [
        "# ***Preprocessing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9DPx7j_g4wq"
      },
      "source": [
        "### *Preprocess captions*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jvFoKxWhAwU"
      },
      "source": [
        "def load_captions(filename):\n",
        "    \"\"\"\n",
        "    Load captions from file and create a per image caption dictionary\n",
        "    :param filename:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # read from the captions file\n",
        "    file = open(filename, \"r\")\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "\n",
        "    mapping = dict()\n",
        "\n",
        "    # process each line\n",
        "    # line is in form: image_name.jpg#no caption\n",
        "    for line in text.split(\"\\n\"):\n",
        "        token = line.split(\"\\t\")\n",
        "\n",
        "        if len(line) < 2:\n",
        "            continue\n",
        "\n",
        "        # first token: image id\n",
        "        # rest: image caption\n",
        "        img_id, img_capt = token[0], token[1:]\n",
        "        # extract image id: before the .jpg part\n",
        "        img_id = img_id.split('.')[0]\n",
        "        # convert caption list back to string\n",
        "        img_capt = ' '.join(img_capt)\n",
        "\n",
        "        # add all the captions od the same image to image_id key\n",
        "        if img_id not in mapping:\n",
        "            mapping[img_id] = list()\n",
        "        mapping[img_id].append(img_capt)\n",
        "\n",
        "    return mapping\n",
        "\n",
        "\n",
        "def clean_captions(captions):\n",
        "    \"\"\"\n",
        "    Remove punctuation, hanging s and a, and tokens with numbers\n",
        "    from the captions\n",
        "    :param captions:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # Prepare translation table for removing punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for _, caption_list in captions.items():\n",
        "        for i in range(len(caption_list)):\n",
        "            caption = caption_list[i]\n",
        "            # Tokenize i.e. split on white spaces\n",
        "            caption = caption.split()\n",
        "            # Convert to lowercase\n",
        "            caption = [word.lower() for word in caption]\n",
        "            # Remove punctuation from each token\n",
        "            caption = [w.translate(table) for w in caption]\n",
        "            # Remove hanging 's' and 'a'\n",
        "            caption = [word for word in caption if len(word)>1]\n",
        "            # Remove tokens with numbers in them\n",
        "            caption = [word for word in caption if word.isalpha()]\n",
        "            # Store as string\n",
        "            caption_list[i] = ' '.join(caption)\n",
        "\n",
        "\n",
        "def save_captions(captions_dict, to_file):\n",
        "    \"\"\"\n",
        "    Save the captions_dict to a file,\n",
        "    file: image_id caption_list per line\n",
        "    :param captions_dict:\n",
        "    :param to_file:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # convert captions dictionary to string of lines\n",
        "    lines = list()\n",
        "    for key, caption_list in captions_dict.items():\n",
        "        for caption in caption_list:\n",
        "            lines.append(key + ' ' + caption)\n",
        "    data = '\\n'.join(lines)\n",
        "\n",
        "    # save captions string to a file\n",
        "    file = open(to_file, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "\n",
        "\n",
        "def preprocess_captions(capt_filename=f\"{drive_folder}/Dataset/Flickr8k_text/Flickr8k.token.txt\",\n",
        "                        clean_capt_to_file=f\"{drive_folder}/training_files/captions.txt\"):\n",
        "    captions_dict = load_captions(capt_filename)\n",
        "    clean_captions(captions_dict)\n",
        "    save_captions(captions_dict, clean_capt_to_file)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BF60d1KhzXM"
      },
      "source": [
        "preprocess_captions()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz80tODNh8Uz"
      },
      "source": [
        "### *Extract image features*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUWZ2EpmkvTA"
      },
      "source": [
        "def create_cnn_model_dict():\n",
        "  cnn_model_dict = dict()\n",
        "\n",
        "  cnn_model_dict['vgg16'] = {\n",
        "      'model': vgg16.VGG16(),\n",
        "      'target_size': (224, 224),\n",
        "      'preprocess_input': vgg16.preprocess_input\n",
        "  }\n",
        "\n",
        "  cnn_model_dict['inception_v3'] = {\n",
        "      'model': inception_v3.InceptionV3(),\n",
        "      'target_size': (299, 299),\n",
        "      'preprocess_input': inception_v3.preprocess_input\n",
        "  }\n",
        "  return cnn_model_dict"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atNRjTOiNvhn"
      },
      "source": [
        "def extract_features(images_dir, model_type, cnn_model_dict):\n",
        "    model = cnn_model_dict[model_type]['model']\n",
        "    target_size = cnn_model_dict[model_type]['target_size']\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "    model.summary()\n",
        "\n",
        "    features_dict = dict()\n",
        "\n",
        "    img_count = 0\n",
        "\n",
        "    for name in listdir(images_dir):\n",
        "      filename = f\"{images_dir}/{name}\"\n",
        "      image = load_img(filename, target_size=target_size)\n",
        "      image = img_to_array(image)\n",
        "      image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "      image = cnn_model_dict[model_type]['preprocess_input'](image)\n",
        "      features = model.predict(image, verbose=0)\n",
        "      image_id = name.split('.')[0]\n",
        "      features_dict[image_id] = features\n",
        "      \n",
        "      img_count += 1\n",
        "\n",
        "      if img_count % 200 == 0:\n",
        "        print(\"No. images\", img_count)\n",
        "        print()\n",
        "\n",
        "      print(\".\", end=\"\")\n",
        "\n",
        "    return features_dict\n",
        "\n",
        "def save_img_features(img_features, to_file):\n",
        "  dump(img_features, open(to_file, \"wb\"))\n",
        "\n",
        "def preprocess_img_features(images_dir=f\"{drive_folder}/Dataset/Flickr8k_Dataset/Flicker8k_Dataset\",\n",
        "                            to_file=f\"{drive_folder}/training_files/img_features.pkl\",\n",
        "                            model_type):\n",
        "  cnn_model_dict = create_cnn_model_dict()\n",
        "  features = extract_features(images_dir, model_type, cnn_model_dict)\n",
        "  print(\"No. features\", len(features))\n",
        "  save_img_features(features, to_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipc70N3viD4f"
      },
      "source": [
        "preprocess_img_features(to_file=f\"{drive_folder}/training_files/img_features_inc_v3.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EOZidkRHO44"
      },
      "source": [
        "# ***Load preprocessed data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7BKYa48MmnZ"
      },
      "source": [
        "def load_img_ids(filename):\n",
        "    \"\"\"\n",
        "    Load image ids from a file\n",
        "    \"\"\"\n",
        "    file = open(filename, \"r\")\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "\n",
        "    img_ids = list()\n",
        "    for line in text.split(\"\\n\"):\n",
        "\n",
        "        if len(line) < 1:\n",
        "            continue\n",
        "\n",
        "        img_id = line.split('.')[0]\n",
        "        img_ids.append(img_id)\n",
        "\n",
        "    return img_ids"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emO6ZllHJWVB"
      },
      "source": [
        "def load_img_features(img_features, train_ids, test_ids):\n",
        "    \"\"\"\n",
        "    Load train and test features from a file\n",
        "    :param img_features:\n",
        "    :param train_ids:\n",
        "    :param test_ids:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    features = load(open(img_features, \"rb\"))\n",
        "\n",
        "    train_features = {train_id: features[train_id] for train_id in train_ids}\n",
        "    test_features = {test_id: features[test_id] for test_id in test_ids}\n",
        "\n",
        "    return train_features, test_features\n",
        "\n",
        "def load_clean_captions(filename, dataset):\n",
        "    \"\"\"\n",
        "    load captions from file and create entry for each imgId from dataset\n",
        "    \"\"\"\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "\n",
        "    captions = dict()\n",
        "\n",
        "    for line in text.split('\\n'):\n",
        "\n",
        "        tokens = line.split()\n",
        "        img_id, img_caption = tokens[0], tokens[1:]\n",
        "\n",
        "        if img_id in dataset:\n",
        "            if img_id not in captions:\n",
        "                captions[img_id] = list()\n",
        "\n",
        "            # add startseq at the begining and endseq at the end of each caption\n",
        "            caption = 'startseq ' + ' '.join(img_caption) + ' endseq'\n",
        "            captions[img_id].append(caption)\n",
        "\n",
        "    return captions"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4vw6Kt78jFp"
      },
      "source": [
        "def load_train_test(img_features_path, captions_path, train_ids_path, test_ids_path):\n",
        "    \"\"\"\n",
        "    Load train image features and captions, load test image features and captions\n",
        "    :param img_features_path:\n",
        "    :param captions_path:\n",
        "    :param train_ids_path:\n",
        "    :param test_ids_path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    img_train_ids = load_img_ids(img_train_path)\n",
        "    img_test_ids = load_img_ids(img_test_path)\n",
        "\n",
        "    train_features, test_features = load_img_features(img_features_path, img_train_ids, img_test_ids)\n",
        "   \n",
        "    train_captions = load_clean_captions(captions_filename, img_train_ids)\n",
        "    test_captions = load_clean_captions(captions_filename, img_test_ids)\n",
        "\n",
        "    print(\"Train images: \", len(train_features))\n",
        "    print(\"Train captions: \", len(train_captions))\n",
        "    print(\"Test images: \", len(test_features))\n",
        "    print(\"Test captions: \", len(test_captions))\n",
        "\n",
        "    return train_features, train_captions, test_features, test_captions\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZRiXRIM8PRN",
        "outputId": "1e3f81d7-59a9-44b9-b1ac-eaad0889d579"
      },
      "source": [
        "train_features, train_captions, test_features, test_captions = load_train_test(img_features_path, captions_filename, img_train_path, img_test_path)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train images:  6000\n",
            "Train captions:  6000\n",
            "Test images:  1000\n",
            "Test captions:  1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzxWpbdG9bj3"
      },
      "source": [
        "# ***Prepare data for model fitting***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgIunbfPlxlR"
      },
      "source": [
        "def to_lines(captions):\n",
        "    \"\"\"\n",
        "    Extract values from captions dictionary\n",
        "    \"\"\"\n",
        "    all_captions = list()\n",
        "    for key in captions.keys():\n",
        "        [all_captions.append(d) for d in captions[key]]\n",
        "    return all_captions\n",
        "\n",
        "\n",
        "def create_tokenizer(captions):\n",
        "    lines = to_lines(captions)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def calc_max_length(captions):\n",
        "    lines = to_lines(captions)\n",
        "    return max(len(line.split()) for line in lines)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKVtDgWnnf48",
        "outputId": "d88d0e61-2c5d-464a-8275-20ebb92e5238"
      },
      "source": [
        "tokenizer = create_tokenizer(train_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary size: \", vocab_size)\n",
        "max_length = calc_max_length(train_captions)\n",
        "print(\"Max caption length: \", max_length)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  7579\n",
            "Max caption length:  34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmLSkiwRmlr-"
      },
      "source": [
        "def create_sequences(image, caption_list, tokenizer, max_length, vocab_size):\n",
        "    \"\"\"\n",
        "    Generate sequences from a caption, containing just the first word, first two words etc.\n",
        "    For word i in sequence, separate the caption into input=caption[:i] and next_word=caption[i];\n",
        "    encode each word as a categorical value.\n",
        "    :param image:\n",
        "    :param caption_list:\n",
        "    :param tokenizer:\n",
        "    :param max_length:\n",
        "    :param vocab_size:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    in_img_list, in_word_list, out_word_list = list(), list(), list()\n",
        "    for capt in caption_list:\n",
        "        # tokenize each caption\n",
        "        seq = tokenizer.texts_to_sequences([capt])[0]\n",
        "        for i in range(1, len(seq)):\n",
        "            in_seq, out_seq = seq[:i], seq[i]\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "            # encode word to a categorical value\n",
        "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\n",
        "            in_img_list.append(image)\n",
        "            in_word_list.append(in_seq)\n",
        "            out_word_list.append(out_seq)\n",
        "    return in_img_list, in_word_list, out_word_list"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw51_mtNnvIW"
      },
      "source": [
        "def data_generator(images, captions, tokenizer, max_length, batch_size, random_seed, vocab_size):\n",
        "    \"\"\"\n",
        "    Extract images, input word sequences and output word in batches. To be used while fitting the model.\n",
        "    :param images:\n",
        "    :param captions:\n",
        "    :param tokenizer:\n",
        "    :param max_length:\n",
        "    :param batch_size:\n",
        "    :param random_seed:\n",
        "    :param vocab_size:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    random.seed(random_seed)\n",
        "\n",
        "    img_ids = list(captions.keys())\n",
        "\n",
        "    count = 0\n",
        "    while True:\n",
        "        if count >= len(img_ids):\n",
        "            count = 0\n",
        "\n",
        "        in_img_batch, in_seq_batch, out_word_batch = list(), list(), list()\n",
        "\n",
        "        # get current batch indexes\n",
        "        for i in range(count, min(len(img_ids), count+batch_size)):\n",
        "            # current image_id\n",
        "            img_id = img_ids[i]\n",
        "            # current image\n",
        "            img = images[img_id][0]\n",
        "            # current image caption list\n",
        "            captions_list = captions[img_id]\n",
        "            # shuffle the captions\n",
        "            random.shuffle(captions_list)\n",
        "            # get word sequences and output word\n",
        "            in_img, in_seq, out_word = create_sequences(img, captions_list, tokenizer, max_length, vocab_size)\n",
        "\n",
        "            # append to batch list\n",
        "            for j in range(len(in_img)):\n",
        "                in_img_batch.append(in_img[j])\n",
        "                in_seq_batch.append(in_seq[j])\n",
        "                out_word_batch.append(out_word[j])\n",
        "\n",
        "        count = count + batch_size\n",
        "        yield [np.array(in_img_batch), np.array(in_seq_batch)], np.array(out_word_batch)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpDlihdm_Y8K",
        "outputId": "55baa97c-f074-40a7-9230-4c9c5b1fee4d"
      },
      "source": [
        "generator = data_generator(train_features, train_captions, tokenizer, max_length, 1, 10, vocab_size)\n",
        "input = next(generator)\n",
        "print(input[0][0].shape)\n",
        "print(input[0][1].shape)\n",
        "print(input[1].shape)\n",
        "print(len(input))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(47, 4096)\n",
            "(47, 34)\n",
            "(47, 7579)\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AECSo2iDYSb"
      },
      "source": [
        "# ***Model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBjjEf-CDbqC"
      },
      "source": [
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length):\t\n",
        "  image_input = Input(shape=(4096,))\n",
        "  image_model_1 = Dropout(0.5)(image_input)\n",
        "  image_model = Dense(256, activation='relu')(image_model_1)\n",
        "\n",
        "  caption_input = Input(shape=(max_length,))\n",
        "\t# mask_zero: We zero pad inputs to the same length, the zero mask ignores those inputs. E.g. it is an efficiency.\n",
        "  caption_model_1 = Embedding(vocab_size, 256, mask_zero=True)(caption_input)\n",
        "  caption_model_2 = Dropout(0.5)(caption_model_1)\n",
        "  caption_model = LSTM(256)(caption_model_2)\n",
        "\n",
        "\t# Merging the models and creating a softmax classifier\n",
        "  final_model_1 = concatenate([image_model, caption_model])\n",
        "  final_model_2 = Dense(256, activation='relu')(final_model_1)\n",
        "  final_model = Dense(vocab_size, activation='softmax')(final_model_2)\n",
        "\n",
        "  model = Model(inputs=[image_input, caption_input], outputs=final_model)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxvo29n4DoQt",
        "outputId": "2e9368e4-8567-4672-d08c-a5a97e7ae23a"
      },
      "source": [
        "model = define_model(vocab_size, max_length)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_15 (InputLayer)           [(None, 34)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_14 (InputLayer)           [(None, 4096)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 34, 256)      1940224     input_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 4096)         0           input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 34, 256)      0           embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 256)          1048832     dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   (None, 256)          525312      dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 512)          0           dense_17[0][0]                   \n",
            "                                                                 lstm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 256)          131328      concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, 7579)         1947803     dense_18[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 5,593,499\n",
            "Trainable params: 5,593,499\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xql2_Nfmg_N"
      },
      "source": [
        "def plot_history(history):\n",
        "    plt.plot(history['loss'])\n",
        "    plt.plot(history['val_loss'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss through epochs')\n",
        "    plt.legend(['Train', 'Test'], loc='best')"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVyxBV3XUH5r"
      },
      "source": [
        "def train_model(model, epochs, batch_size, plot_hist=True):\n",
        "  train_steps = len(train_captions) // batch_size\n",
        "  if len(train_captions) % batch_size != 0:\n",
        "    train_steps = train_steps +1\n",
        "\n",
        "  test_steps = len(test_captions) // batch_size\n",
        "  if len(test_captions) % batch_size != 0:\n",
        "    test_steps = test_steps + 1\n",
        "\n",
        "  filepath = drive_folder + \"/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "  train_generator = data_generator(train_features, train_captions, tokenizer, max_length, batch_size, 10, vocab_size)\n",
        "  test_generator = data_generator(test_features, test_captions, tokenizer, max_length, batch_size, 10, vocab_size)\n",
        "\n",
        "  history = model.fit(train_generator, epochs=epochs, steps_per_epoch=train_steps,\n",
        "            validation_data=test_generator, validation_steps=test_steps,\n",
        "            callbacks=[checkpoint], verbose=1)\n",
        "  \n",
        "  if plot_hist:\n",
        "    plot_history(history.history)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcbTH-7EWKn-"
      },
      "source": [
        "train_model(model, 2, 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7y6jHz2cHFK"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "\n",
        "def index_to_word(word_index, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == word_index:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "\n",
        "def generate_caption(model, tokenizer, image, max_length):\n",
        "    in_caption = \"startseq\"\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        seq = tokenizer.texts_to_sequences([in_caption])[0]\n",
        "\n",
        "        seq = pad_sequences([seq], maxlen=max_length)\n",
        "\n",
        "        pred = model.predict([image, seq], verbose=0)\n",
        "\n",
        "        pred = np.argmax(pred)\n",
        "\n",
        "        word = index_to_word(pred, tokenizer)\n",
        "\n",
        "        if word is None:\n",
        "            break\n",
        "\n",
        "        in_caption += ' ' + word\n",
        "\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "\n",
        "    return in_caption\n",
        "\n",
        "\n",
        "def evaluate_model(model, images, captions, tokenizer, max_lenght):\n",
        "    test, predicted = list(), list()\n",
        "\n",
        "    img_count = 0\n",
        "    for key, caption_list in captions.items():\n",
        "        pred = generate_caption(model, tokenizer, images[key], max_lenght)\n",
        "\n",
        "        predicted.append(pred.split())\n",
        "        test.append([capt.split() for capt in caption_list])\n",
        "\n",
        "        img_count += 1\n",
        "\n",
        "        if img_count % 200 == 0:\n",
        "          print(\"No. images\", img_count)\n",
        "          print()\n",
        "\n",
        "        print(\".\", end=\"\")\n",
        "        \n",
        "    print(\"BLEU-1 \", corpus_bleu(test, predicted, weights=(1.0, 0, 0, 0)))\n",
        "    print(\"BLEU-2 \", corpus_bleu(test, predicted, weights=(.5, .5, 0, 0)))\n",
        "    print(\"BLEU-3 \", corpus_bleu(test, predicted, weights=(.3, .3, .3, 0)))\n",
        "    print(\"BLEU-4 \", corpus_bleu(test, predicted))"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJo3CKHicIST",
        "outputId": "3cb05d4a-e8d8-4d57-80d3-cddce17d1d04"
      },
      "source": [
        "from keras.models import load_model\n",
        "test_model = load_model(f\"{drive_folder}/model-ep006-loss3.256-val_loss3.879.h5\")\n",
        "evaluate_model(test_model, test_features, test_captions, tokenizer, max_length)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".......................................................................................................................................................................................................No. images 200\n",
            "\n",
            "........................................................................................................................................................................................................No. images 400\n",
            "\n",
            "........................................................................................................................................................................................................No. images 600\n",
            "\n",
            "........................................................................................................................................................................................................No. images 800\n",
            "\n",
            "........................................................................................................................................................................................................No. images 1000\n",
            "\n",
            ".BLEU-1  0.5840836479814115\n",
            "BLEU-2  0.3592187301590539\n",
            "BLEU-3  0.2619139901673539\n",
            "BLEU-4  0.13870722005485744\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}