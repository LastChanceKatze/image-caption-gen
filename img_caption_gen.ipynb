{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "img-caption-gen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LastChanceKatze/image-caption-gen/blob/main/img_caption_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEHDWYoigbhC"
      },
      "source": [
        "# ***Imports***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhheFCoXN3v2"
      },
      "source": [
        "from os import listdir\n",
        "import string\n",
        "from pickle import dump, load\n",
        "import tensorflow.keras.applications.vgg16 as vgg16\n",
        "import tensorflow.keras.applications.inception_v3 as inception_v3\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTsgYyp4L4rJ",
        "outputId": "19f0ee42-1f44-4495-fdd3-4c600c802191"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02GJ0XL47wBr"
      },
      "source": [
        "drive_folder = \"/content/drive/MyDrive/DL\"\n",
        "vgg_img_features_path = f\"{drive_folder}/training_files/img_features.pkl\"\n",
        "inc_img_features_path = f\"{drive_folder}/training_files/img_features_inc_v3.pkl\"\n",
        "img_train_path = f\"{drive_folder}/Dataset/Flickr8k_text/Flickr_8k.trainImages.txt\"\n",
        "img_test_path = f\"{drive_folder}/Dataset/Flickr8k_text/Flickr_8k.devImages.txt\"\n",
        "captions_filename = f\"{drive_folder}/training_files/captions.txt\"\n",
        "glove_path = f\"{drive_folder}/glove/glove.txt\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEy8DbU3giqo"
      },
      "source": [
        "# ***Preprocessing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9DPx7j_g4wq"
      },
      "source": [
        "### *Preprocess captions*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jvFoKxWhAwU"
      },
      "source": [
        "def load_captions(filename):\n",
        "    \"\"\"\n",
        "    Load captions from file and create a per image caption dictionary\n",
        "    :param filename:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # read from the captions file\n",
        "    file = open(filename, \"r\")\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "\n",
        "    mapping = dict()\n",
        "\n",
        "    # process each line\n",
        "    # line is in form: image_name.jpg#no caption\n",
        "    for line in text.split(\"\\n\"):\n",
        "        token = line.split(\"\\t\")\n",
        "\n",
        "        if len(line) < 2:\n",
        "            continue\n",
        "\n",
        "        # first token: image id\n",
        "        # rest: image caption\n",
        "        img_id, img_capt = token[0], token[1:]\n",
        "        # extract image id: before the .jpg part\n",
        "        img_id = img_id.split('.')[0]\n",
        "        # convert caption list back to string\n",
        "        img_capt = ' '.join(img_capt)\n",
        "\n",
        "        # add all the captions od the same image to image_id key\n",
        "        if img_id not in mapping:\n",
        "            mapping[img_id] = list()\n",
        "        mapping[img_id].append(img_capt)\n",
        "\n",
        "    return mapping\n",
        "\n",
        "\n",
        "def clean_captions(captions):\n",
        "    \"\"\"\n",
        "    Remove punctuation, hanging s and a, and tokens with numbers\n",
        "    from the captions\n",
        "    :param captions:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # Prepare translation table for removing punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for _, caption_list in captions.items():\n",
        "        for i in range(len(caption_list)):\n",
        "            caption = caption_list[i]\n",
        "            # Tokenize i.e. split on white spaces\n",
        "            caption = caption.split()\n",
        "            # Convert to lowercase\n",
        "            caption = [word.lower() for word in caption]\n",
        "            # Remove punctuation from each token\n",
        "            caption = [w.translate(table) for w in caption]\n",
        "            # Remove hanging 's' and 'a'\n",
        "            caption = [word for word in caption if len(word)>1]\n",
        "            # Remove tokens with numbers in them\n",
        "            caption = [word for word in caption if word.isalpha()]\n",
        "            # Store as string\n",
        "            caption_list[i] = ' '.join(caption)\n",
        "\n",
        "\n",
        "def save_captions(captions_dict, to_file):\n",
        "    \"\"\"\n",
        "    Save the captions_dict to a file,\n",
        "    file: image_id caption_list per line\n",
        "    :param captions_dict:\n",
        "    :param to_file:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # convert captions dictionary to string of lines\n",
        "    lines = list()\n",
        "    for key, caption_list in captions_dict.items():\n",
        "        for caption in caption_list:\n",
        "            lines.append(key + ' ' + caption)\n",
        "    data = '\\n'.join(lines)\n",
        "\n",
        "    # save captions string to a file\n",
        "    file = open(to_file, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "\n",
        "\n",
        "def preprocess_captions(capt_filename=f\"{drive_folder}/Dataset/Flickr8k_text/Flickr8k.token.txt\",\n",
        "                        clean_capt_to_file=f\"{drive_folder}/training_files/captions.txt\"):\n",
        "    captions_dict = load_captions(capt_filename)\n",
        "    clean_captions(captions_dict)\n",
        "    save_captions(captions_dict, clean_capt_to_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BF60d1KhzXM"
      },
      "source": [
        "preprocess_captions()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz80tODNh8Uz"
      },
      "source": [
        "### *Extract image features*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUWZ2EpmkvTA"
      },
      "source": [
        "def create_cnn_model_dict():\n",
        "  cnn_model_dict = dict()\n",
        "\n",
        "  cnn_model_dict['vgg16'] = {\n",
        "      'model': vgg16.VGG16(),\n",
        "      'target_size': (224, 224),\n",
        "      'preprocess_input': vgg16.preprocess_input\n",
        "  }\n",
        "\n",
        "  cnn_model_dict['inception_v3'] = {\n",
        "      'model': inception_v3.InceptionV3(),\n",
        "      'target_size': (299, 299),\n",
        "      'preprocess_input': inception_v3.preprocess_input\n",
        "  }\n",
        "  return cnn_model_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atNRjTOiNvhn"
      },
      "source": [
        "def extract_features(images_dir, model_type, cnn_model_dict):\n",
        "    model = cnn_model_dict[model_type]['model']\n",
        "    target_size = cnn_model_dict[model_type]['target_size']\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "    model.summary()\n",
        "\n",
        "    features_dict = dict()\n",
        "\n",
        "    img_count = 0\n",
        "\n",
        "    for name in listdir(images_dir):\n",
        "      filename = f\"{images_dir}/{name}\"\n",
        "      image = load_img(filename, target_size=target_size)\n",
        "      image = img_to_array(image)\n",
        "      image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "      image = cnn_model_dict[model_type]['preprocess_input'](image)\n",
        "      features = model.predict(image, verbose=0)\n",
        "      image_id = name.split('.')[0]\n",
        "      features_dict[image_id] = features\n",
        "      \n",
        "      img_count += 1\n",
        "\n",
        "      if img_count % 200 == 0:\n",
        "        print(\"No. images\", img_count)\n",
        "        print()\n",
        "\n",
        "      print(\".\", end=\"\")\n",
        "\n",
        "    return features_dict\n",
        "\n",
        "def save_img_features(img_features, to_file):\n",
        "  dump(img_features, open(to_file, \"wb\"))\n",
        "\n",
        "def preprocess_img_features(images_dir=f\"{drive_folder}/Dataset/Flickr8k_Dataset/Flicker8k_Dataset\",\n",
        "                            to_file=f\"{drive_folder}/training_files/img_features.pkl\",\n",
        "                            model_type):\n",
        "  cnn_model_dict = create_cnn_model_dict()\n",
        "  features = extract_features(images_dir, model_type, cnn_model_dict)\n",
        "  print(\"No. features\", len(features))\n",
        "  save_img_features(features, to_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipc70N3viD4f"
      },
      "source": [
        "preprocess_img_features(to_file=f\"{drive_folder}/training_files/img_features_inc_v3.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EOZidkRHO44"
      },
      "source": [
        "# ***Load preprocessed data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7BKYa48MmnZ"
      },
      "source": [
        "def load_img_ids(filename):\n",
        "    \"\"\"\n",
        "    Load image ids from a file\n",
        "    \"\"\"\n",
        "    file = open(filename, \"r\")\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "\n",
        "    img_ids = list()\n",
        "    for line in text.split(\"\\n\"):\n",
        "\n",
        "        if len(line) < 1:\n",
        "            continue\n",
        "\n",
        "        img_id = line.split('.')[0]\n",
        "        img_ids.append(img_id)\n",
        "\n",
        "    return img_ids"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emO6ZllHJWVB"
      },
      "source": [
        "def load_img_features(img_features, train_ids, test_ids):\n",
        "    \"\"\"\n",
        "    Load train and test features from a file\n",
        "    :param img_features:\n",
        "    :param train_ids:\n",
        "    :param test_ids:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    features = load(open(img_features, \"rb\"))\n",
        "\n",
        "    train_features = {train_id: features[train_id] for train_id in train_ids}\n",
        "    test_features = {test_id: features[test_id] for test_id in test_ids}\n",
        "\n",
        "    return train_features, test_features\n",
        "\n",
        "def load_clean_captions(filename, dataset):\n",
        "    \"\"\"\n",
        "    load captions from file and create entry for each imgId from dataset\n",
        "    \"\"\"\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "\n",
        "    captions = dict()\n",
        "\n",
        "    for line in text.split('\\n'):\n",
        "\n",
        "        tokens = line.split()\n",
        "        img_id, img_caption = tokens[0], tokens[1:]\n",
        "\n",
        "        if img_id in dataset:\n",
        "            if img_id not in captions:\n",
        "                captions[img_id] = list()\n",
        "\n",
        "            # add startseq at the begining and endseq at the end of each caption\n",
        "            caption = 'startseq ' + ' '.join(img_caption) + ' endseq'\n",
        "            captions[img_id].append(caption)\n",
        "\n",
        "    return captions"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4vw6Kt78jFp"
      },
      "source": [
        "# NOT USED\n",
        "def load_train_test(img_features_path, captions_path, train_ids_path, \n",
        "                    test_ids_path):\n",
        "    \"\"\"\n",
        "    Load train image features and captions, load test image features and captions\n",
        "    :param img_features_path:\n",
        "    :param captions_path:\n",
        "    :param train_ids_path:\n",
        "    :param test_ids_path:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    img_train_ids = load_img_ids(img_train_path)\n",
        "    img_test_ids = load_img_ids(img_test_path)\n",
        "\n",
        "    train_features, test_features = load_img_features(img_features_path, img_train_ids, img_test_ids)\n",
        "\n",
        "\n",
        "    train_captions = load_clean_captions(captions_filename, img_train_ids)\n",
        "    test_captions = load_clean_captions(captions_filename, img_test_ids)\n",
        "\n",
        "    print(\"Train images: \", len(train_features))\n",
        "    print(\"Train captions: \", len(train_captions))\n",
        "    print(\"Test images: \", len(test_features))\n",
        "    print(\"Test captions: \", len(test_captions))\n",
        "\n",
        "    return train_features, train_captions, test_features, test_captions\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzNyAlE7oNk9"
      },
      "source": [
        "img_train_ids = load_img_ids(img_train_path)\n",
        "img_test_ids = load_img_ids(img_test_path)\n",
        "\n",
        "train_captions = load_clean_captions(captions_filename, img_train_ids)\n",
        "test_captions = load_clean_captions(captions_filename, img_test_ids)\n",
        "\n",
        "vgg_train_features, vgg_test_features = load_img_features(vgg_img_features_path, img_train_ids, img_test_ids)\n",
        "inc_train_features, inc_test_features = load_img_features(inc_img_features_path, img_train_ids, img_test_ids)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRQPzDIXplFm",
        "outputId": "7fce01c0-e78c-423d-d62b-b045b1a441d0"
      },
      "source": [
        "print(\"Train images: \", len(vgg_train_features))\n",
        "print(\"Train captions: \", len(train_captions))\n",
        "print(\"Test images: \", len(vgg_test_features))\n",
        "print(\"Test captions: \", len(test_captions))\n",
        "print(\"VGG-16 feature vector:\", len(vgg_train_features[img_train_ids[0]][0]))\n",
        "print(\"Inception V3 feature vector:\", len(inc_train_features[img_train_ids[0]][0]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train images:  6000\n",
            "Train captions:  6000\n",
            "Test images:  1000\n",
            "Test captions:  1000\n",
            "VGG-16 feature vector: 4096\n",
            "Inception V3 feature vector: 2048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzxWpbdG9bj3"
      },
      "source": [
        "# ***Prepare data for model fitting***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgIunbfPlxlR"
      },
      "source": [
        "def to_lines(captions):\n",
        "    \"\"\"\n",
        "    Extract values from captions dictionary\n",
        "    \"\"\"\n",
        "    all_captions = list()\n",
        "    for key in captions.keys():\n",
        "        [all_captions.append(d) for d in captions[key]]\n",
        "    return all_captions\n",
        "\n",
        "\n",
        "def create_tokenizer(captions):\n",
        "    lines = to_lines(captions)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def calc_max_length(captions):\n",
        "    lines = to_lines(captions)\n",
        "    return max(len(line.split()) for line in lines)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKVtDgWnnf48",
        "outputId": "cb9e9cf6-a7b7-42ae-8690-7470ae179622"
      },
      "source": [
        "tokenizer = create_tokenizer(train_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary size: \", vocab_size)\n",
        "max_length = calc_max_length(train_captions)\n",
        "print(\"Max caption length: \", max_length)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  7579\n",
            "Max caption length:  34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmLSkiwRmlr-"
      },
      "source": [
        "def create_sequences(image, caption_list, tokenizer, max_length, vocab_size):\n",
        "    \"\"\"\n",
        "    Generate sequences from a caption, containing just the first word, first two words etc.\n",
        "    For word i in sequence, separate the caption into input=caption[:i] and next_word=caption[i];\n",
        "    encode each word as a categorical value.\n",
        "    :param image:\n",
        "    :param caption_list:\n",
        "    :param tokenizer:\n",
        "    :param max_length:\n",
        "    :param vocab_size:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    in_img_list, in_word_list, out_word_list = list(), list(), list()\n",
        "    for capt in caption_list:\n",
        "        # tokenize each caption\n",
        "        seq = tokenizer.texts_to_sequences([capt])[0]\n",
        "        for i in range(1, len(seq)):\n",
        "            in_seq, out_seq = seq[:i], seq[i]\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "            # encode word to a categorical value\n",
        "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\n",
        "            in_img_list.append(image)\n",
        "            in_word_list.append(in_seq)\n",
        "            out_word_list.append(out_seq)\n",
        "    return in_img_list, in_word_list, out_word_list"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw51_mtNnvIW"
      },
      "source": [
        "def data_generator(images, captions, tokenizer, max_length, batch_size, random_seed, vocab_size):\n",
        "    \"\"\"\n",
        "    Extract images, input word sequences and output word in batches. To be used while fitting the model.\n",
        "    :param images:\n",
        "    :param captions:\n",
        "    :param tokenizer:\n",
        "    :param max_length:\n",
        "    :param batch_size:\n",
        "    :param random_seed:\n",
        "    :param vocab_size:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    random.seed(random_seed)\n",
        "\n",
        "    img_ids = list(captions.keys())\n",
        "\n",
        "    count = 0\n",
        "    while True:\n",
        "        if count >= len(img_ids):\n",
        "            count = 0\n",
        "\n",
        "        in_img_batch, in_seq_batch, out_word_batch = list(), list(), list()\n",
        "\n",
        "        # get current batch indexes\n",
        "        for i in range(count, min(len(img_ids), count+batch_size)):\n",
        "            # current image_id\n",
        "            img_id = img_ids[i]\n",
        "            # current image\n",
        "            img = images[img_id][0]\n",
        "            # current image caption list\n",
        "            captions_list = captions[img_id]\n",
        "            # shuffle the captions\n",
        "            random.shuffle(captions_list)\n",
        "            # get word sequences and output word\n",
        "            in_img, in_seq, out_word = create_sequences(img, captions_list, tokenizer, max_length, vocab_size)\n",
        "\n",
        "            # append to batch list\n",
        "            for j in range(len(in_img)):\n",
        "                in_img_batch.append(in_img[j])\n",
        "                in_seq_batch.append(in_seq[j])\n",
        "                out_word_batch.append(out_word[j])\n",
        "\n",
        "        count = count + batch_size\n",
        "        yield [np.array(in_img_batch), np.array(in_seq_batch)], np.array(out_word_batch)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkUvRxaAhH8Z"
      },
      "source": [
        "#***Glove embedding***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyEWxHi7hLsY"
      },
      "source": [
        "def glove_embedding_indices(glove_filepath):\n",
        "    \"\"\"\n",
        "    Mapira world embedding fajl u dictionary, za svaku rec u fajlu\n",
        "     mapira njene relacije\n",
        "    :param word_index:\n",
        "    :param vocab_size:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    embeddings_index = {}\n",
        "\n",
        "    glove = open(glove_filepath, 'r', encoding='utf-8').read()\n",
        "    for line in glove.split(\"\\n\"):\n",
        "        values = line.split(\" \")\n",
        "        word = values[0]\n",
        "        indices = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = indices\n",
        "\n",
        "    return embeddings_index\n",
        "\n",
        "def get_vocab_embedding_weights(embeddings_index, word_index, vocab_size, emd_dim=200):\n",
        "    \"\"\"\n",
        "    Kreira embedding matricu za reci koje\n",
        "     se javljaju u vocab_index\n",
        "    :param embeddings_index:\n",
        "    :param word_index:\n",
        "    :param vocab_size: \n",
        "    :return: \n",
        "    \"\"\"\n",
        "    #emb_dim = 200\n",
        "    emb_matrix = np.zeros((vocab_size, emb_dim))\n",
        "    for word, i in word_index.items():\n",
        "        emb_vec = embeddings_index.get(word)\n",
        "        if emb_vec is not None:\n",
        "            emb_matrix[i] = emb_vec\n",
        "\n",
        "    return emb_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "PFN0ImpchizE",
        "outputId": "1106e815-ceaa-41b3-c5c4-08d191d63cb8"
      },
      "source": [
        " emb_dim = 200\n",
        "\n",
        " def get_embedding_weights():\n",
        "    embedding_matrix = glove_embedding_indices(glove_path)\n",
        "    embedding_matrix_weights = get_vocab_embedding_weights(embedding_matrix, tokenizer.word_index, vocab_size, emb_dim)\n",
        "    return embedding_matrix_weights\n",
        "\n",
        "embedding_weights = get_embedding_weights()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e3a07f5c7905>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0membedding_matrix_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0membedding_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-e3a07f5c7905>\u001b[0m in \u001b[0;36mget_embedding_weights\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_embedding_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m    \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglove_embedding_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m    \u001b[0membedding_matrix_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vocab_embedding_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0membedding_matrix_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'glove_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AECSo2iDYSb"
      },
      "source": [
        "# ***Model***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9vSMPXC0xZ2"
      },
      "source": [
        "**Basic model - vgg16**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBjjEf-CDbqC"
      },
      "source": [
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length, params):\t\n",
        "  image_input = Input(shape=(params['input_dim'],))\n",
        "  image_model_1 = Dropout(params['dropout_value'])(image_input)\n",
        "  image_model = Dense(params['dense_units'], activation='relu')(image_model_1)\n",
        "\n",
        "  caption_input = Input(shape=(max_length,))\n",
        "\t# mask_zero: We zero pad inputs to the same length, the zero mask ignores those inputs. E.g. it is an efficiency.\n",
        "  \n",
        "  caption_model_1 = Embedding(vocab_size, params['embedding_size'], mask_zero = True)(caption_input)\n",
        "\n",
        "  caption_model_2 = Dropout(params['dropout_value'])(caption_model_1)\n",
        "  caption_model = LSTM(params['lstm_units'])(caption_model_2)\n",
        "\n",
        "\t# Merging the models and creating a softmax classifier\n",
        "  final_model_1 = concatenate([image_model, caption_model])\n",
        "  final_model_2 = Dense(params['dense_units'], activation='relu')(final_model_1)\n",
        "  final_model = Dense(vocab_size, activation='softmax')(final_model_2)\n",
        "\n",
        "  model = Model(inputs=[image_input, caption_input], outputs=final_model)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq4ZiS6W0dR_"
      },
      "source": [
        "**Model with embedding weights - vgg16**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APYqXuel0cql"
      },
      "source": [
        "# define the captioning model\n",
        "def define_glove_model(vocab_size, max_length, params):\t\n",
        "  image_input = Input(shape=(params['input_dim'],))\n",
        "  image_model_1 = Dropout(params['dropout_value'])(image_input)\n",
        "  image_model = Dense(params['dense_units'], activation='relu')(image_model_1)\n",
        "\n",
        "  caption_input = Input(shape=(max_length,))\n",
        "\t# mask_zero: We zero pad inputs to the same length, the zero mask ignores those inputs. E.g. it is an efficiency.\n",
        "  \n",
        "  caption_model_1 = Embedding(vocab_size, emb_dim, mask_zero = True)(caption_input)\n",
        "\n",
        "  caption_model_2 = Dropout(params['dropout_value'])(caption_model_1)\n",
        "  caption_model = LSTM(params['lstm_units'])(caption_model_2)\n",
        "\n",
        "\t# Merging the models and creating a softmax classifier\n",
        "  final_model_1 = concatenate([image_model, caption_model])\n",
        "  final_model_2 = Dense(params['dense_units'], activation='relu')(final_model_1)\n",
        "  final_model = Dense(vocab_size, activation='softmax')(final_model_2)\n",
        "\n",
        "  model = Model(inputs=[image_input, caption_input], outputs=final_model)\n",
        "\n",
        "  model.layers[2].set_weights([embedding_matrix_weights])\n",
        "  model.layers[2].trainable = False\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiSUtURdF0Yi"
      },
      "source": [
        "def train_model(params):\n",
        "  train_steps = len(params['train_captions']) // params['batch_size'] \n",
        "  if len(params['train_captions']) % params['batch_size']  != 0:\n",
        "    train_steps = train_steps + 1\n",
        "\n",
        "  test_steps = len(params['test_captions']) // params['batch_size']\n",
        "  if len(params['test_captions']) % params['batch_size'] != 0:\n",
        "    test_steps = test_steps + 1\n",
        "\n",
        "  checkpoint = ModelCheckpoint(params['filepath'], monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "  logger = CSVLogger(params['logger'])\n",
        "\n",
        "  train_generator = data_generator(params['train_features'], params['train_captions'], params['tokenizer'], params['max_length'], params['batch_size'], 10, params['vocab_size'])\n",
        "  test_generator = data_generator(params['test_features'], params['test_captions'], params['tokenizer'], params['max_length'], params['batch_size'], 10, params['vocab_size'])\n",
        "\n",
        "  model = params['model']\n",
        "  history = model.fit(train_generator, epochs = params['epochs'], steps_per_epoch = train_steps,\n",
        "            validation_data=test_generator, validation_steps = test_steps,\n",
        "            callbacks=[checkpoint, logger], verbose=1)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li3-OKqJF3C5"
      },
      "source": [
        "def plot_history(log_filepath):\n",
        "    df = pd.read_csv(log_filepath)\n",
        "    plt.plot(df['loss'])\n",
        "    plt.plot(df['val_loss'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss through epochs')\n",
        "    plt.legend(['Train', 'Test'], loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpGyrvhfF8yr"
      },
      "source": [
        "def index_to_word(word_index, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == word_index:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "\n",
        "def generate_caption(model, tokenizer, image, max_length):\n",
        "    in_caption = \"startseq\"\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        seq = tokenizer.texts_to_sequences([in_caption])[0]\n",
        "\n",
        "        seq = pad_sequences([seq], maxlen=max_length)\n",
        "\n",
        "        pred = model.predict([image, seq], verbose=0)\n",
        "\n",
        "        pred = np.argmax(pred)\n",
        "\n",
        "        word = index_to_word(pred, tokenizer)\n",
        "\n",
        "        if word is None:\n",
        "            break\n",
        "\n",
        "        in_caption += ' ' + word\n",
        "\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "\n",
        "    return in_caption\n",
        "\n",
        "\n",
        "def evaluate_model(model, images, captions, tokenizer, max_lenght):\n",
        "    test, predicted = list(), list()\n",
        "\n",
        "    img_count = 0\n",
        "    for key, caption_list in captions.items():\n",
        "        pred = generate_caption(model, tokenizer, images[key], max_lenght)\n",
        "\n",
        "        predicted.append(pred.split())\n",
        "        test.append([capt.split() for capt in caption_list])\n",
        "\n",
        "        img_count += 1\n",
        "\n",
        "        if img_count % 200 == 0:\n",
        "          print(\"No. images\", img_count)\n",
        "          print()\n",
        "\n",
        "        print(\".\", end=\"\")\n",
        "        \n",
        "    print(\"BLEU-1 \", corpus_bleu(test, predicted, weights=(1.0, 0, 0, 0)))\n",
        "    print(\"BLEU-2 \", corpus_bleu(test, predicted, weights=(.5, .5, 0, 0)))\n",
        "    print(\"BLEU-3 \", corpus_bleu(test, predicted, weights=(.3, .3, .3, 0)))\n",
        "    print(\"BLEU-4 \", corpus_bleu(test, predicted))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eV9RhIOGCkW"
      },
      "source": [
        "# VGG + RNN + Dropout(0.5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MafSKJkp1KeK"
      },
      "source": [
        "vgg_rnn_model_params = {\n",
        "    'input_dim': 4096,\n",
        "    'dense_units': 256,\n",
        "    'lstm_units': 256,\n",
        "    'dropout_value':0.5,\n",
        "    'embedding_size': 256\n",
        "     }"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJO6aiO81PZh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68743729-226c-455b-b429-5072d4315dde"
      },
      "source": [
        "vgg_rnn_model = define_model(vocab_size, max_length, vgg_rnn_model_params)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 34)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 4096)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 34, 256)      1940224     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 4096)         0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 34, 256)      0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          1048832     dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 256)          525312      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 512)          0           dense[0][0]                      \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          131328      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 7579)         1947803     dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,593,499\n",
            "Trainable params: 5,593,499\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBKqVj7QHDAm"
      },
      "source": [
        "vgg_rnn_train_params = {'model':vgg_rnn_model,\n",
        "     'epochs': 20,\n",
        "     'batch_size': 64,\n",
        "     'train_captions': train_captions,\n",
        "     'test_captions': test_captions,\n",
        "     'filepath': drive_folder + \"/model/ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5\",\n",
        "     'logger':  f\"{drive_folder}/model/training.log\",\n",
        "     'tokenizer': tokenizer,\n",
        "     'max_length': max_length,    \n",
        "     'vocab_size': vocab_size,\n",
        "     'train_features': vgg_train_features,\n",
        "     'test_features': vgg_test_features\n",
        "     }"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzNoXjidHHU5"
      },
      "source": [
        "train_model(vgg_rnn_train_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-sYowz6HIYt"
      },
      "source": [
        "model_path = f\"{drive_folder}/model/vgg_rnn/ep006-loss3.268-val_loss3.876.h5\"\n",
        "test_model = load_model(model_path)\n",
        "test_model.summary()\n",
        "evaluate_model(test_model, test_features, test_captions, tokenizer, max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IaVBR5kHMqp"
      },
      "source": [
        "## VGG + GLOVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YXlbIaxHR36"
      },
      "source": [
        "define_glove_model_params = {'dropout_value':0.2,\n",
        "     'dense_units': 256,\n",
        "     'input_dim': 4096,\n",
        "     'lstm_units': 256\n",
        "     }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxvo29n4DoQt",
        "outputId": "74c1d1b7-7dfa-4915-d851-8d7959d3afea"
      },
      "source": [
        "model_glove = define_glove_model(vocab_size, max_length, define_glove_model_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 34)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 4096)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 34, 256)      1940224     input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 4096)         0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 256)          1048832     dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 256)          525312      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 512)          0           dense_3[0][0]                    \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 256)          131328      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 7579)         1947803     dense_4[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,593,499\n",
            "Trainable params: 5,593,499\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIy99ZqqigSC"
      },
      "source": [
        "train_model_glove_params = {'model':model_glove,\n",
        "     'epochs': 20,\n",
        "     'batch_size': 64,\n",
        "     'plot_hist': True,\n",
        "     'train_captions': train_captions,\n",
        "     'test_captions': test_captions,\n",
        "     'filepath': drive_folder + \"/glove_model/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5\",\n",
        "     'tokenizer': tokenizer,\n",
        "     'max_length': max_length,    \n",
        "     'vocab_size': vocab_size,\n",
        "     'train_features': train_features,\n",
        "     'test_features': test_features\n",
        "     }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxP8vNPoXLlA"
      },
      "source": [
        "train_model(train_model_glove_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s2Do2rGqjvX"
      },
      "source": [
        "inc_v3_rnn_model_params = {\n",
        "    'input_dim': 2048,\n",
        "    'dense_units': 128,\n",
        "    'lstm_units': 128,\n",
        "    'dropout_value':0.3,\n",
        "    'embedding_size': 128\n",
        "     }\n",
        "inc_v3_rnn_model = define_model(vocab_size, max_length, inc_v3_rnn_model_params)\n",
        "inc_v3_rnn_train_params = {'model':inc_v3_rnn_model,\n",
        "     'epochs': 20,\n",
        "     'batch_size': 64,\n",
        "     'train_captions': train_captions,\n",
        "     'test_captions': test_captions,\n",
        "     'filepath': drive_folder + \"/model/ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5\",\n",
        "     'logger':  f\"{drive_folder}/model/training.log\",\n",
        "     'tokenizer': tokenizer,\n",
        "     'max_length': max_length,    \n",
        "     'vocab_size': vocab_size,\n",
        "     'train_features': inc_train_features,\n",
        "     'test_features': inc_test_features\n",
        "     }\n",
        "train_model(inc_v3_rnn_train_params)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}